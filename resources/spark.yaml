apiVersion: "sparkoperator.k8s.io/v1beta2" #v1beta2-1.3.3-3.1.1?
kind: SparkApplication
metadata:
  name: spark-app # auto name?
  namespace: spark
  labels:
    app: spark-app
spec:
  sparkConf:
    #spark.eventLog.enabled: "true" # spark history
    #spark.eventLog.dir: gs://{{ .Values.buckets.bucket_without_gs}}/{{ .Values.spark_history_name }}
    #spark.sql.streaming.metricsEnabled: "true"
    #spark.ui.prometheus.enabled: "true"
    #spark.hadoop.fs.gs.project.id: {{ .Values.project }} 
    #spark.hadoop.gs.system.bucket: {{ .Values.buckets.bucket_without_gs }}   
   # spark.hadoop.google.cloud.auth.service.account.enable: "true" 



    spark.hadoop.fs.s3a.impl: "org.apache.hadoop.fs.s3a.S3AFileSystem"
    spark.hadoop.fs.s3a.aws.credentials.provider: "com.amazonaws.auth.WebIdentityTokenCredentialsProvider"
    spark.hadoop.fs.s3a.committer.name: magic
    spark.hadoop.fs.s3a.committer.magic.enabled: true
    spark.hadoop.fs.s3a.fast.upload: true




    #spark.hadoop.fs.gs.impl: "com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem"



    spark.hadoop.fs.AbstractFileSystem.gs.impl: "com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS"
    spark.metrics.conf.*.sink.prometheusServlet.class: org.apache.spark.metrics.sink.PrometheusServlet
    spark.metrics.conf.*.sink.prometheusServlet.path: /metrics/prometheus
    spark.metrics.conf.master.sink.prometheusServlet.path: /metrics/master/prometheus
    spark.metrics.conf.applications.sink.prometheusServlet.path: /metrics/applications/prometheus
  
  type: Python
  pythonVersion: "3"
  mode: client
  image: gcr.io/spark/spark:v3.1.1 # make this optional
  imagePullPolicy: Always
  mainApplicationFile: "local:///opt/spark/work-dir/main_app.py" # todo, adjust this to be the right entrypoint
  sparkVersion: "3.1.1"
  
  arguments:
    - "{{ `{{inputs.parameters.appname}}` }}"
    - gs://{{ .Values.buckets.bucket_without_gs }}'
    - "{{ `{{inputs.parameters.table}}` }}"
    - "{{ `{{inputs.parameters.outputs}}` }}"
    - "{{ `{{inputs.parameters.kafka}}` }}"
    - "{{ `{{inputs.parameters.inputs}}` }}"
    - 'checkpoint'
    - "{{ `{{inputs.parameters.version}}` }}"
  
  restartPolicy:
    type: Always # should be able to resume from checkpoint if killed for some reason
  driver:
    coreRequest: 200m
    memory: "512m"
    serviceAccount: {{ .Values.serviceaccounts.spark_service_account.name }} # this maps to spark-gcs 
    labels:
      metrics-exposed: "true"
  executor:
    instances: 1
    cores: 1
    memory: "512m"
    serviceAccount: {{ .Values.serviceaccounts.spark_service_account.name }} # this maps to spark-gcs
    labels:
      metrics-exposed: "true"  